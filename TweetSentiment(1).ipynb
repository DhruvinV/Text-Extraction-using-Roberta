{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt93MqH-lWkc",
        "colab_type": "code",
        "outputId": "4fe4f7ee-c753-4643-e0d5-923d110497d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install regex requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 21.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 56.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=2a3a50d86caf794a3c16f751d11ebc58d5e295198a2ead8acaaec5611b7c8340\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.0\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J1qeMXUPHtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed12ea8c-2b8f-430c-9f88-ee13ee136758"
      },
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Conv2D\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOd89LBKjhvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Loader\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/tweet-sentiment-extraction/'\n",
        "code = \"4/zgHVgt5EYIxQpCKdQTmi61ujDfdn_p673NBszogLYWsCZMJpSbY_WIQ\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL9Ing6AMSXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def read_test():\n",
        "  path = root_path + \"test.csv\"\n",
        "  df = pd.read_csv(path)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  return shuffle(df,random_state=np.random.randint(0,10))\n",
        "def reaf_submission():\n",
        "    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
        "    return shuffle(test,random_state=np.random.randint(0,10))\n",
        "def read_train():\n",
        "  path = root_path + \"train.csv\"\n",
        "  df = pd.read_csv(path)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  df['selected_text']=df['selected_text'].astype(str)\n",
        "  return shuffle(df,random_state=np.random.randint(0,10))\n",
        "\n",
        "print(read_train().sample(frac=1).head(20).index.tolist)\n",
        "train_csv, test = read_train().sample(frac=1).reset_index(drop=True), read_test().sample(frac=1).reset_index(drop=True)\n",
        "train, val = train_test_split(train_csv, test_size=0.12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCN6C6aYqiXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metric tested \n",
        "def jaccard(str1,str2):\n",
        "  a = set(str1.lower().split()) \n",
        "  b = set(str2.lower().split())\n",
        "  c = a.intersection(b)\n",
        "  return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPakYDyDne2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "currModel = 'roberta-base'\n",
        "# using both to simplfy various methods\n",
        "tokenzier = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)\n",
        "tokenzier.save_vocabulary(root_path)\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=root_path+'vocab.json', \n",
        "    merges_file=root_path+'merges.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "# these are the ids used by our tokenizer\n",
        "sentiment = {\n",
        "    'neutral': int(tokenizer.encode('neutral').ids[0]),\n",
        "    'positive': int(tokenizer.encode('positive').ids[0]),\n",
        "    'negative':int(tokenizer.encode('negative').ids[0])\n",
        "}\n",
        "print(sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROVeSP9MBI4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The structure of our input would be of the following form [Text={Context},special_token,sentiment={Question}]\n",
        "# which is then predicted with selected_text.\n",
        "def get_input_ids(idf):\n",
        "    input_ids = []\n",
        "    for i in range(idf.shape[0]):\n",
        "        context,label = idf['text'].iloc[i].split(),idf['sentiment'].iloc[i].split()\n",
        "        encoded_input = tokenzier.encode_plus(\n",
        "                            \" \".join(context),\" \".join(label),                   # Sentence to encode.\n",
        "                            # Add '[CLS]' and '[SEP]' \n",
        "                    )\n",
        "        # encode_input returns input_ids and attention masks. \n",
        "        # Masks would be helpful latter to instruct model to avoid \n",
        "        # special tokens such as padding. \n",
        "        input_ids.append(encoded_input)\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m28T-l0Q8DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_set_ids = get_input_ids(train)\n",
        "val_set_ids = get_input_ids(val)\n",
        "test_ids = get_input_ids(test_csv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV-dG3fpIv3H",
        "colab_type": "text"
      },
      "source": [
        "The next part of data pre-processing is to come up with start and end tokens which gives start index and end index of the selected word in our encoded input. \n",
        "\n",
        "For instance, Conside the sample with\n",
        "\n",
        "text = \"Sooo SAD have to leave boston\"\n",
        "\n",
        "selected_text = \"Sooo SAD\" \n",
        "\n",
        "In this case start token is S and end token is D \n",
        "input_ids = [0, 3, 3, 11990, 560, 38457, 3, 2, 2, 2430, 2]\n",
        "\n",
        "start_token = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "end_token = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "Note: There are two ways this can be done character level and token level. I'm using character level tokenization in order to prevent loss of training samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y7Q7YEdUJY4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1_Ym_dy0eWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(input_ids[3])\n",
        "def create_labels(train2,t_ids):\n",
        "    st = []\n",
        "    et = []\n",
        "    for i in range(train2.shape[0]):\n",
        "        text,label = \" \".join(train2['text'].iloc[i].split()), \" \".join(train2['selected_text'].iloc[i].split())\n",
        "        char = np.zeros((len(text)))\n",
        "        idx = text.find(label)\n",
        "        char[idx:idx+len(label)] = 1\n",
        "        if text[idx-1]==' ': char[idx-1] = 1\n",
        "        enc = tokenizer.encode(text)\n",
        "        offsets = enc.offsets\n",
        "        chard_to_token_index = []\n",
        "        start = list(char).index(1)+1\n",
        "        end_idx = start+len(label)\n",
        "        # print(start,end_idx)\n",
        "        for k in range(len(offsets)):\n",
        "            my_set = range(offsets[k][0],offsets[k][1])\n",
        "            # print(my_set)\n",
        "            if(start <= offsets[k][1] and  offsets[k][1] <= end_idx or (start in my_set and end_idx in my_set)):\n",
        "                chard_to_token_index.append(k)\n",
        "        start_tokens,end_tokens = [0]*len(t_ids[i].input_ids),[0]*len(t_ids[i].input_ids)\n",
        "        if(len(chard_to_token_index) > 1):\n",
        "            # print(i)\n",
        "            start_tokens[chard_to_token_index[0]+1] = 1\n",
        "            end_tokens[chard_to_token_index[1]+1] = 1\n",
        "            st.append(start_tokens)\n",
        "            et.append(end_tokens)\n",
        "        else:\n",
        "            start_tokens[chard_to_token_index[0]+1],end_tokens[chard_to_token_index[0]+1] = 1,1\n",
        "            st.append(start_tokens)\n",
        "            et.append(end_tokens)\n",
        "    return st,et\n",
        "train_st,train_et = create_labels(train,train_set_ids)\n",
        "val_st,val_et = create_labels(val,val_set_ids)\n",
        "# test_st,test_et = create_labels(test_csv,train_set_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTsXsnKyUKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given the sequence length lets set MAX_LEN to 100\n",
        "MAX_LEN = 100\n",
        "def padding_input(ids,max_len,mask=False):\n",
        "    print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenzier.pad_token, tokenzier.pad_token_id))\n",
        "# seperate attention_masks and input_ids.\n",
        "    print(ids[0])\n",
        "    train_ids = [seq.input_ids for seq in ids]\n",
        "    attention_masks = [seq.attention_mask for seq in ids]\n",
        "    # input_ids = None\n",
        "    train_ids = pad_sequences(train_ids, maxlen=MAX_LEN, dtype=\"long\",value=1, truncating=\"post\", padding=\"post\")\n",
        "    if mask:\n",
        "        attention_masks = pad_sequences(attention_masks, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "        \n",
        "        return (train_ids,attention_masks)\n",
        "    return train_ids\n",
        "train_ids, train_masks = padding_input(train_set_ids,MAX_LEN,True)\n",
        "val_ids,val_masks = padding_input(val_set_ids,MAX_LEN,True)\n",
        "test_ids,test_masks = padding_input(test_ids,MAX_LEN,True)\n",
        "# Converting to torch tensors in order to use GPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3vLf_T4o-5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = {\n",
        "    \"ids\": torch.tensor(train_ids,dtype=torch.long),\n",
        "    \"att_mask\": torch.tensor(train_masks,dtype=torch.long),\n",
        "    \"token_type_ids\": torch.zeros((train.shape[0],MAX_LEN),dtype=torch.long)\n",
        "}\n",
        "val_data = {\n",
        "    \"ids\": torch.tensor(val_ids,dtype=torch.long),\n",
        "    \"att_mask\": torch.tensor(val_masks,dtype=torch.long),\n",
        "    \"token_type_ids\": torch.zeros((val.shape[0],MAX_LEN),dtype=torch.long)\n",
        "\n",
        "}\n",
        "test_data = {\n",
        "    \"ids\": torch.tensor(test_ids,dtype=torch.long),\n",
        "    \"att_mask\": torch.tensor(test_masks,dtype=torch.long),\n",
        "    \"token_type_ids\": torch.zeros((test.shape[0],MAX_LEN),dtype=torch.long)\n",
        "}\n",
        "train_label = {\n",
        "    \"start_tokens\":torch.tensor(pad_sequences(train_st,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "    \"end_tokens\": torch.tensor(pad_sequences(train_et,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "}\n",
        "val_label = {\n",
        "    \"start_tokens\":torch.tensor(pad_sequences(val_st,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "    \"end_tokens\": torch.tensor(pad_sequences(val_et,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcIrDf4GVeXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the DataLoader for our training set.\n",
        "print(val_data[\"ids\"].shape,val_data[\"att_mask\"].shape,val_data[\"token_type_ids\"].shape)\n",
        "print(val_label[\"start_tokens\"].shape,val_label[\"end_tokens\"].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f9zqCArzQ6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_data[\"ids\"], train_data[\"att_mask\"], train_data[\"token_type_ids\"],train_label[\"start_tokens\"],train_label[\"end_tokens\"])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(val_data[\"ids\"], val_data[\"att_mask\"], val_data[\"token_type_ids\"],val_label[\"start_tokens\"],val_label[\"end_tokens\"])\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSIENFdN4Dot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data pre-processing is one of the most important tasks when it comes to NLP. Next includes model \n",
        "# selection and training in order to make prediction on this new task.\n",
        "# Model\n",
        "from transformers import RobertaModel,RobertaConfig\n",
        "class SentimentAnalysis(nn.Module):\n",
        "    def __init__(self,configs):\n",
        "        super(SentimentAnalysis,self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(root_path+'roberta-base-weights.bin',config=configs)\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Conv1d(768,768,kernel_size=1)  \n",
        "        )\n",
        "        self.avgpool  =  nn.AvgPool1d(kernel_size=5)\n",
        "        self.linear = nn.Linear(153,2)\n",
        "        self.linear2 = nn.Linear(2,1)\n",
        "    def forward(self,token_ids,att_masks,type_ids):\n",
        "        outputs = self.roberta(\n",
        "            input_ids=token_ids,\n",
        "            attention_mask=att_masks,\n",
        "            token_type_ids=type_ids,\n",
        "            )\n",
        "        bs = outputs[0]\n",
        "        sl = self.container(torch.transpose(bs,dim0=1,dim1=2))\n",
        "        avgpool = self.avgpool(torch.transpose(sl,dim0=1,dim1=2))\n",
        "        start_logits = self.linear2(nn.functional.softmax(self.linear(avgpool),dim=2))\n",
        "        el = self.container(torch.transpose(bs,dim0=1,dim1=2))\n",
        "        avgpool = self.avgpool(self.avgpool(torch.transpose(el,dim0=1,dim1=2)))\n",
        "        end_logits = self.linear2(nn.function.softmax(self.linear(avgpool),dim=2))\n",
        "        start_logits = nn.functional.softmax(torch.flatten(start_logits,start_dim=1,end_dim=2),dim=1)\n",
        "        end_logits = nn.functional.softmax(torch.flatten(start_logits,start_dim=1,end_dim=2),dim=1)\n",
        "        return start_logits,end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl3raGFWJbdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SentimentAnalysis(configs=root_path+'config.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgxvLxxciIzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "# print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONk3qk6XamGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(prediction,target):\n",
        "    diff_in_start = nn.CrossEntropyLoss(prediction[0],target[0])\n",
        "    diff_in_end  = nn.CrossEntropyLoss(predition[1],target[1])\n",
        "    return diff_in_start+diff_in_end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zi9T_IFFZ0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "import random\n",
        "import time\n",
        "seed_val = 40\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "def train_model(model):\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
        "                  weight_decay = 0.1\n",
        "                )\n",
        "    epochs = 5\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                        num_warmup_steps = 0, \n",
        "                                        num_training_steps = total_steps)\n",
        "    loss_values = []\n",
        "    for epoch_i in range(0, epochs):\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "        t0 = time.time()\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "            tok_ids = batch[0].to(device)\n",
        "            atten_masks = batch[1].to(device)\n",
        "            tok_type_ids = batch[2].to(device)\n",
        "            st_tok = batch[3].to(device)\n",
        "            et_tok = batch[4].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            start_logits, end_logits = model(token_ids=tok_ids,\n",
        "                        att_masks=atten_masks,type_ids=tok_type_ids)\n",
        "            loss = calculate_loss((st_tok,et_tok),start_logits,end_logits)\n",
        "            total_loss = torch.sum(x).item()\n",
        "            loss.backward()\n",
        "        # clip grad to prevent exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        # update learning rate\n",
        "        scheduler.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(train_dataloader)  \n",
        "        loss_values.append(avg_train_loss)\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        print(\"Running Validation...\")\n",
        "        t0 = time.time()\n",
        "        model.eval()\n",
        "        val_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            v_input_ids, v_input_mask, v_tok_ids,v_st,v_et = batch\n",
        "            with torch.no_grad():        \n",
        "                start_logits, end_logits = model(token_ids=v_input_ids,\n",
        "                        att_masks=v_input_mask,type_ids=v_tok_ids)\n",
        "\n",
        "            start_logits = start_logits.detach().cpu().numpy()\n",
        "            end_logits = end_logits.detach().cpu().numpy()\n",
        "            val_st = v_st.to('cpu').numpy()\n",
        "            val_et = v_et.to('cpu').numpy()\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = calculate_loss((start_logits,end_logits), (val_st,val_et))\n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    print(\" \")\n",
        "    print(\"Training complete!\")\n",
        "    return loss_values\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY4HJvFaHFkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freeze = train_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}