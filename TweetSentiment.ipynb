{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "TweetSetniments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt93MqH-lWkc",
        "outputId": "e0ba3cd5-8b62-4666-e484-2a0cd0913a52",
        "trusted": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install regex requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J1qeMXUPHtq",
        "outputId": "fc53c0de-8232-4f7d-e0f3-e40e046416a0",
        "trusted": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "from transformers import RobertaModel,RobertaConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOd89LBKjhvc",
        "trusted": true,
        "colab_type": "code",
        "outputId": "a3117c12-89c4-40f5-cb4d-628d546bfe8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "# Data Loader\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/tweet-sentiment-extraction/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL9Ing6AMSXR",
        "trusted": true,
        "colab_type": "code",
        "outputId": "88438b44-8a93-49ad-e4eb-5e400ad47d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "\n",
        "\n",
        "def read_test():\n",
        "  paths = root_path+\"test.csv\"\n",
        "  df = pd.read_csv(paths)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  return shuffle(df,random_state=np.random.randint(0,10))\n",
        "def reaf_submission():\n",
        "    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
        "    return shuffle(test,random_state=np.random.randint(0,10))\n",
        "def read_train():\n",
        "  paths = root_path+\"train.csv\"\n",
        "  df = pd.read_csv(paths)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  df['selected_text']=df['selected_text'].astype(str)\n",
        "  return shuffle(df,random_state=np.random.randint(0,10))\n",
        "\n",
        "print(read_train().sample(frac=1).head(20).index.tolist)\n",
        "train_csv, test_org = read_train().sample(frac=1).reset_index(drop=True), read_test().sample(frac=1).reset_index(drop=True)\n",
        "train, val = train_test_split(train_csv, test_size=0.12)\n",
        "train, test_csv = train_test_split(train, test_size=0.03)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method IndexOpsMixin.tolist of Int64Index([ 1069,  7890, 13745, 26428,  9965, 15382, 17022,   572, 26498,\n",
            "             6739, 21819, 11569, 26332, 24904, 23416, 21725,  9548, 19582,\n",
            "            19464, 15337],\n",
            "           dtype='int64')>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCN6C6aYqiXa",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metric tested \n",
        "def jaccard(str1,str2):\n",
        "  a = set(str1.lower().split()) \n",
        "  b = set(str2.lower().split())\n",
        "  c = a.intersection(b)\n",
        "  return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPakYDyDne2U",
        "trusted": true,
        "colab_type": "code",
        "outputId": "f9643079-5f52-4acd-b92b-1a19fb0347d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "currModel = 'roberta-base'\n",
        "# using both to simplfy various methods\n",
        "# tokenzier = RobertaTokenizer.from_pretrained(root_path,do_lower_case=True)\n",
        "tokenzier = RobertaTokenizer.from_pretrained(currModel,do_lower_case=True)\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=root_path+'vocab.json', \n",
        "    merges_file=root_path+'merges.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "# these are the ids used by our tokenizer\n",
        "sentiment = {\n",
        "    'neutral': int(tokenizer.encode('neutral').ids[0]),\n",
        "    'positive': int(tokenizer.encode('positive').ids[0]),\n",
        "    'negative':int(tokenizer.encode('negative').ids[0])\n",
        "}\n",
        "print(sentiment)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'neutral': 7974, 'positive': 1313, 'negative': 2430}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROVeSP9MBI4n",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The structure of our input would be of the following form [Text={Context},special_token,sentiment={Question}]\n",
        "# which is then predicted with selected_text.\n",
        "def get_input_ids(idf):\n",
        "    input_ids = []\n",
        "    for i in range(idf.shape[0]):\n",
        "        context,label = idf['text'].iloc[i].split(),idf['sentiment'].iloc[i].split()\n",
        "        encoded_input = tokenzier.encode_plus(\n",
        "                            \" \".join(context),\" \".join(label),                   # Sentence to encode.\n",
        "                            # Add '[CLS]' and '[SEP]' \n",
        "                    )\n",
        "        # encode_input returns input_ids and attention masks. \n",
        "        # Masks would be helpful latter to instruct model to avoid \n",
        "        # special tokens such as padding. \n",
        "        input_ids.append(encoded_input)\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m28T-l0Q8DR",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# train_set_ids = get_input_ids(train)\n",
        "# val_set_ids = get_input_ids(val)\n",
        "test_ids = get_input_ids(test_csv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV-dG3fpIv3H",
        "colab_type": "text"
      },
      "source": [
        "The next part of data pre-processing is to come up with start and end tokens which gives start index and end index of the selected word in our encoded input. \n",
        "\n",
        "For instance, Conside the sample with\n",
        "\n",
        "text = \"Sooo SAD have to leave boston\"\n",
        "\n",
        "selected_text = \"Sooo SAD\" \n",
        "\n",
        "In this case start token is S and end token is D \n",
        "input_ids = [0, 3, 3, 11990, 560, 38457, 3, 2, 2, 2430, 2]\n",
        "\n",
        "start_token = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "end_token = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "Note: There are two ways this can be done character level and token level. I'm using character level tokenization in order to prevent loss of training samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y7Q7YEdUJY4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1_Ym_dy0eWg",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(input_ids[3])\n",
        "def create_labels(train2,t_ids):\n",
        "    st = []\n",
        "    et = []\n",
        "    for i in range(train2.shape[0]):\n",
        "        text,label = \" \".join(train2['text'].iloc[i].split()), \" \".join(train2['selected_text'].iloc[i].split())\n",
        "        char = np.zeros((len(text)))\n",
        "        idx = text.find(label)\n",
        "        char[idx:idx+len(label)] = 1\n",
        "        if text[idx-1]==' ': char[idx-1] = 1\n",
        "        enc = tokenizer.encode(text)\n",
        "        offsets = enc.offsets\n",
        "        chard_to_token_index = []\n",
        "        start = list(char).index(1)+1\n",
        "        end_idx = start+len(label)\n",
        "        # print(start,end_idx)\n",
        "        for k in range(len(offsets)):\n",
        "            my_set = range(offsets[k][0],offsets[k][1])\n",
        "            # print(my_set)\n",
        "            if(start <= offsets[k][1] and  offsets[k][1] <= end_idx or (start in my_set and end_idx in my_set)):\n",
        "                chard_to_token_index.append(k)\n",
        "        start_tokens,end_tokens = [0]*len(t_ids[i].input_ids),[0]*len(t_ids[i].input_ids)\n",
        "        if(len(chard_to_token_index) > 1):\n",
        "            # print(i)\n",
        "            start_tokens[chard_to_token_index[0]+1] = 1\n",
        "            end_tokens[chard_to_token_index[1]+1] = 1\n",
        "            st.append(start_tokens)\n",
        "            et.append(end_tokens)\n",
        "        else:\n",
        "            start_tokens[chard_to_token_index[0]+1],end_tokens[chard_to_token_index[0]+1] = 1,1\n",
        "            st.append(start_tokens)\n",
        "            et.append(end_tokens)\n",
        "    return st,et\n",
        "# train_st,train_et = create_labels(train,train_set_ids)\n",
        "# val_st,val_et = create_labels(val,val_set_ids) \n",
        "# test_st,test_et = create_labels(test_csv,train_set_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTsXsnKyUKD",
        "trusted": true,
        "colab_type": "code",
        "outputId": "756aa7bf-a48e-4771-cea7-501a91bd55f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Given the sequence length lets set MAX_LEN to 100\n",
        "MAX_LEN = 100\n",
        "def padding_input(ids,max_len,mask=False):\n",
        "    print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenzier.pad_token, tokenzier.pad_token_id))\n",
        "# seperate attention_masks and input_ids.\n",
        "    print(ids[0])\n",
        "    train_ids = [seq.input_ids for seq in ids]\n",
        "    attention_masks = [seq.attention_mask for seq in ids]\n",
        "    # input_ids = None\n",
        "    train_ids = pad_sequences(train_ids, maxlen=MAX_LEN, dtype=\"long\",value=1, truncating=\"post\", padding=\"post\")\n",
        "    if mask:\n",
        "        attention_masks = pad_sequences(attention_masks, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "        \n",
        "        return (train_ids,attention_masks)\n",
        "    return train_ids\n",
        "# train_ids, train_masks = padding_input(train_set_ids,MAX_LEN,True)\n",
        "# val_ids,val_masks = padding_input(val_set_ids,MAX_LEN,True)\n",
        "test_ids,test_masks = padding_input(test_ids,MAX_LEN,True) \n",
        "# Converting to torch tensors in order to use GPU."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding token: \"<pad>\", ID: 1\n",
            "{'input_ids': [0, 1368, 5471, 734, 24, 12905, 29, 10, 16829, 47, 12905, 241, 634, 7409, 6, 8, 24, 12905, 29, 1531, 734, 98, 734, 3545, 328, 2, 2, 1313, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3vLf_T4o-5F",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(train_ids.shape)\n",
        "# train_data = {\n",
        "#     \"ids\": torch.tensor(train_ids,dtype=torch.long),\n",
        "#     \"att_mask\": torch.tensor(train_masks,dtype=torch.long),\n",
        "#     \"token_type_ids\": torch.zeros((train.shape[0],MAX_LEN),dtype=torch.long)\n",
        "# }\n",
        "# val_data = {\n",
        "#     \"ids\": torch.tensor(val_ids,dtype=torch.long),\n",
        "#     \"att_mask\": torch.tensor(val_masks,dtype=torch.long),\n",
        "#     \"token_type_ids\": torch.zeros((val.shape[0],MAX_LEN),dtype=torch.long)\n",
        "\n",
        "# }\n",
        "test_data = {\n",
        "    \"ids\": torch.tensor(test_ids,dtype=torch.long),\n",
        "    \"att_mask\": torch.tensor(test_masks,dtype=torch.long),\n",
        "    \"token_type_ids\": torch.zeros((test_csv.shape[0],MAX_LEN),dtype=torch.long)\n",
        "}\n",
        "# train_label = {\n",
        "#     \"start_tokens\":torch.tensor(pad_sequences(train_st,maxlen=MAX_LEN, dtype=\"long\", \n",
        "#                             value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "#     \"end_tokens\":torch.tensor(pad_sequences(train_et,maxlen=MAX_LEN, dtype=\"long\", \n",
        "#                             value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "# }\n",
        "# val_label = {\n",
        "#     \"start_tokens\":torch.tensor(pad_sequences(val_st,maxlen=MAX_LEN, dtype=\"long\", \n",
        "#                             value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "#     \"end_tokens\": torch.tensor(pad_sequences(val_et,maxlen=MAX_LEN, dtype=\"long\", \n",
        "#                             value=0, truncating=\"post\", padding=\"post\"),dtype=torch.long),\n",
        "# }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcIrDf4GVeXs",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the DataLoader for our training set.\n",
        "# print(val_data[\"ids\"].shape,val_data[\"att_mask\"].shape,val_data[\"token_type_ids\"].shape)\n",
        "# print(val_label[\"start_tokens\"].shape,val_label[\"end_tokens\"].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f9zqCArzQ6n",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment only for retraining \n",
        "# batch_size = 32\n",
        "# train_data = TensorDataset(train_data[\"ids\"], train_data[\"att_mask\"], train_data[\"token_type_ids\"],train_label[\"start_tokens\"],train_label[\"end_tokens\"])\n",
        "# train_sampler = RandomSampler(train_data)\n",
        "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "# # Create the DataLoader for our validation set.\n",
        "# validation_data = TensorDataset(val_data[\"ids\"], val_data[\"att_mask\"], val_data[\"token_type_ids\"],val_label[\"start_tokens\"],val_label[\"end_tokens\"])\n",
        "# validation_sampler = SequentialSampler(validation_data)\n",
        "# validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSIENFdN4Dot",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data pre-processing is one of the most important tasks when it comes to NLP. Next includes model \n",
        "# selection and training in order to make prediction on this new task.\n",
        "# Model\n",
        "\n",
        "class SentimentAnalysis(nn.Module):\n",
        "    def __init__(self,configs):\n",
        "        super(SentimentAnalysis,self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(root_path+'roberta-base-weights.bin',config=configs)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=1,stride=(1,4))\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Conv1d(768*2,768,kernel_size=1),\n",
        "            nn.Softmax(dim=-1),\n",
        "            \n",
        "        )\n",
        "        self.linear = nn.Linear(768,2)\n",
        "    def forward(self,token_ids,att_masks,type_ids):\n",
        "        _,_,y = self.roberta(\n",
        "            input_ids=token_ids,\n",
        "            attention_mask=att_masks,\n",
        "            token_type_ids=type_ids,\n",
        "            )\n",
        "        avg_pool = torch.squeeze(self.avgpool(torch.stack((y[-2],y[-3],y[-4],y[-5]),dim=-1)),dim=-1)\n",
        "        stacked = torch.transpose(torch.cat((y[-1],avg_pool),dim=-1),dim0=1,dim1=2)\n",
        "        contained = self.container(stacked)\n",
        "        start_logits,end_logits = self.linear(torch.transpose(contained,dim0=1,dim1=2)).split(1, dim=-1) \n",
        "        start_logits = start_logits.squeeze(-1) # (bs x SL)\n",
        "        end_logits = end_logits.squeeze(-1) # (bs x SL)\n",
        "        return start_logits,end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl3raGFWJbdb",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SentimentAnalysis(configs=root_path+'config.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgxvLxxciIzA",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# device = torch.device(\"cuda\")\n",
        "# model.to(device)\n",
        "# # print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONk3qk6XamGc",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(prediction,target):\n",
        "    some_loss = nn.CrossEntropyLoss()\n",
        "    diff_in_start = some_loss(prediction[0],target[0])\n",
        "    diff_in_end  = some_loss(prediction[1],target[1])\n",
        "    return (diff_in_start+diff_in_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zi9T_IFFZ0q",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def format_time(elapsed):\n",
        "#     elapsed_rounded = int(round((elapsed)))\n",
        "#     return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# seed_val = 40\n",
        "# random.seed(seed_val)\n",
        "# np.random.seed(seed_val)\n",
        "# torch.manual_seed(seed_val)\n",
        "# torch.cuda.manual_seed_all(seed_val)\n",
        "# def train_model(model,TEST_SENTENCE):\n",
        "#     optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 3e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "#                   eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
        "#                   weight_decay = 0\n",
        "#                 )\n",
        "#     epochs = 5\n",
        "#     total_steps = len(train_dataloader) * epochs\n",
        "#     scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "#                                         num_warmup_steps = 0, \n",
        "#                                         num_training_steps = total_steps)\n",
        "#     loss_values = []\n",
        "#     for epoch_i in range(0, epochs):\n",
        "#         print(\"\")\n",
        "#         print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "#         print('Training...')\n",
        "#         t0 = time.time()\n",
        "#         total_loss = 0\n",
        "#         model.train()\n",
        "        \n",
        "#         for step, batch in enumerate(train_dataloader):\n",
        "#             if step % 32 == 0 and not step == 0:\n",
        "#                 elapsed = format_time(time.time() - t0)\n",
        "#                 print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "#             tok_ids = batch[0].to(device)\n",
        "#             atten_masks = batch[1].to(device)\n",
        "#             tok_type_ids = batch[2].to(device)\n",
        "#             st_tok = torch.max(batch[3],1)[1].to(device)\n",
        "#             # .to(device)\n",
        "#             et_tok = torch.max(batch[4],1)[1].to(device)\n",
        "\n",
        "#             model.zero_grad()\n",
        "\n",
        "#             start_logits, end_logits = model(token_ids=tok_ids,\n",
        "#                         att_masks=atten_masks,type_ids=tok_type_ids)\n",
        "#             loss = calculate_loss((start_logits,end_logits),(st_tok,et_tok))\n",
        "#             total_loss += torch.sum(loss).item()\n",
        "#             loss.backward()\n",
        "#         # clip grad to prevent exploding gradient problem\n",
        "#         # Update parameters\n",
        "#             optimizer.step()\n",
        "#         # update learning rate\n",
        "#             scheduler.step()\n",
        "#     # Calculate the average loss over the training data.\n",
        "#         avg_train_loss = total_loss / len(train_dataloader)  \n",
        "#         loss_values.append(avg_train_loss)\n",
        "#         print(\"\")\n",
        "#         print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "#         print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "#         print(\"Running Validation...\")\n",
        "#         t0 = time.time()\n",
        "#         model.eval()\n",
        "#         val_loss, eval_accuracy = 0, 0\n",
        "#         nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "#         for batch in validation_dataloader:\n",
        "#             v_input_ids = batch[0].to(device)\n",
        "#             v_input_mask =  batch[1].to(device)\n",
        "#             v_tok_ids = batch[2].to(device)\n",
        "#             v_st  = torch.max(batch[3],1)[1].to(device)\n",
        "#             v_et = torch.max(batch[4],1)[1].to(device)\n",
        "#             with torch.no_grad():        \n",
        "#                 start_logits, end_logits = model(token_ids=v_input_ids,\n",
        "#                         att_masks=v_input_mask,type_ids=v_tok_ids)\n",
        "\n",
        "# #             start_logits = start_logits.detach().cpu().numpy()\n",
        "# #             end_logits = end_logits.detach().cpu().numpy()\n",
        "# #             val_st = v_st.to('cpu').numpy()\n",
        "# #             val_et = v_et.to('cpu').numpy()\n",
        "#             # Calculate the accuracy for this batch of test sentences.\n",
        "#             tmp_eval_accuracy = calculate_loss((start_logits,end_logits),(v_st,v_et))\n",
        "#             # Accumulate the total accuracy.\n",
        "#             eval_accuracy += tmp_eval_accuracy\n",
        "#             # Track the number of batches\n",
        "#             nb_eval_steps += 1\n",
        "#         print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "#         print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "#     print(\" \")\n",
        "#     print(\"Training complete!\")\n",
        "#     return loss_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY4HJvFaHFkq",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze = train_model(model,\"leggo\")\n",
        "# torch.save(model.state_dict(),root_path+\"baseline/baseline\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUoTSMGcUhdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load((root_path+\"baseline/baseline\")))\n",
        "device = torch.device(\"cuda\")\n",
        "# model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STbkLkdmugiD",
        "colab_type": "code",
        "outputId": "9910c13a-9ecb-412f-a7a5-8cc804d34039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "\n",
        "raw_data = test_csv\n",
        "jaccard_output = []\n",
        "text_output = []\n",
        "print(test_data[\"ids\"].shape)\n",
        "test_data[\"ids\"].cuda(),\n",
        "test_data[\"att_mask\"].cuda()\n",
        "test_data[\"token_type_ids\"].cuda()\n",
        "y= model(test_data[\"ids\"],test_data[\"att_mask\"],test_data[\"token_type_ids\"])\n",
        "print(y[0].shape,y[1].shape)\n",
        "for i in range(raw_data.shape[0]):\n",
        "    start = torch.argmax(y[0][i])\n",
        "    end = torch.argmax(y[1][i])\n",
        "    text1 = \" \"+\" \".join(raw_data['text'].iloc[k].split())\n",
        "    enc = tokenizer.encode(text1)\n",
        "    st = tokenizer.decode(enc.ids[a-1:b+1])\n",
        "    text_output.append(st)\n",
        "    jaccard_output.append(jaccard(raw_data['selected_text'].iloc[k],st))\n",
        "print(raw_data['text'].iloc[k],text_output)\n",
        "print(np.mean(jaccard_output))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([726, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhfriWee1pQa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}