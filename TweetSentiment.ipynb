{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt93MqH-lWkc",
        "colab_type": "code",
        "outputId": "d6c82a3b-7f81-49da-c08c-ca0d2f88ad06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J1qeMXUPHtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Conv2D\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOd89LBKjhvc",
        "colab_type": "code",
        "outputId": "28f5de5e-cde5-4ffe-b55a-a40b66231832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Data Loader\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/tweet-sentiment-extraction/'\n",
        "code = \"4/zgHVgt5EYIxQpCKdQTmi61ujDfdn_p673NBszogLYWsCZMJpSbY_WIQ\""
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL9Ing6AMSXR",
        "colab_type": "code",
        "outputId": "d4d099eb-3776-4f3e-8a34-af548fa457d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def read_test():\n",
        "  path = root_path + \"test.csv\"\n",
        "  df = pd.read_csv(path)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  return df\n",
        "def reaf_submission():\n",
        "    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
        "    return test\n",
        "\n",
        "def read_train():\n",
        "  path = root_path + \"train.csv\"\n",
        "  df = pd.read_csv(path)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  df['selected_text']=df['selected_text'].astype(str)\n",
        "  return df\n",
        "\n",
        "train, test = read_train(), read_test()\n",
        "samples = train.sample(1)\n",
        "print(samples)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          textID  ... sentiment\n",
            "5612  85c482fcbf  ...   neutral\n",
            "\n",
            "[1 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCN6C6aYqiXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function specifically requested for this\n",
        "\n",
        "def jaccard(str1,str2):\n",
        "  a = set(str1.lower().split()) \n",
        "  b = set(str2.lower().split())\n",
        "  c = a.intersection(b)\n",
        "  return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPakYDyDne2U",
        "colab_type": "code",
        "outputId": "2da73b63-2510-4ddb-cca2-34d999732c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        "from transformers import *\n",
        "import tokenizers\n",
        " \n",
        "# using both to simplfy various methods\n",
        "tokenzier = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)\n",
        "tokenzier.save_vocabulary(root_path)\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=root_path+'vocab.json', \n",
        "    merges_file=root_path+'merges.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "# these are the ids used by our tokenizer\n",
        "sentiment = {\n",
        "    'neutral': int(tokenizer.encode('neutral').ids[0]),\n",
        "    'positive': int(tokenizer.encode('positive').ids[0]),\n",
        "    'negative':int(tokenizer.encode('negative').ids[0])\n",
        "}\n",
        "print(sentiment)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'neutral': 7974, 'positive': 1313, 'negative': 2430}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROVeSP9MBI4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The structure of our input would be of the following form [Text={Context},special_token,sentiment={Question}]\n",
        "# which is then predicted with selected_text.\n",
        "def get_input_ids(df):\n",
        "    input_ids = []\n",
        "    for i in range(df.shape[0]):\n",
        "        context,label = df.loc[i,'text'].split(),df.loc[i,'sentiment'].split()\n",
        "        encoded_input = tokenzier.encode_plus(\n",
        "                            \" \".join(context),\" \".join(label),                   # Sentence to encode.\n",
        "                            # Add '[CLS]' and '[SEP]' \n",
        "                    )\n",
        "        # encode_input returns input_ids and attention masks. \n",
        "        # Masks would be helpful latter to instruct model to avoid \n",
        "        # special tokens such as padding. \n",
        "        input_ids.append(encoded_input)\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq3EIAqXxlBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = get_input_ids(train)\n",
        "test_ids = get_input_ids(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV-dG3fpIv3H",
        "colab_type": "text"
      },
      "source": [
        "The next part of data pre-processing is to come up with start and end tokens which gives start index and end index of the selected word in our encoded input. \n",
        "\n",
        "For instance, Conside the sample with\n",
        "\n",
        "text = \"Sooo SAD have to leave boston\"\n",
        "\n",
        "selected_text = \"Sooo SAD\" \n",
        "\n",
        "In this case start token is S and end token is D \n",
        "input_ids = [0, 3, 3, 11990, 560, 38457, 3, 2, 2, 2430, 2]\n",
        "\n",
        "start_token = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "end_token = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "Note: There are two ways this can be done character level and token level. I'm using character level tokenization in order to prevent loss of training samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y7Q7YEdUJY4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1_Ym_dy0eWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(input_ids[3])\n",
        "start_tokens = [0] * train.shape[0]\n",
        "end_tokens = [0] * train.shape[0]\n",
        "for i in range(train.shape[0]):\n",
        "    text,label = \" \".join(train.loc[i,'text'].split()), \" \".join(train.loc[i,'selected_text'].split())\n",
        "    char = np.zeros((len(text)))\n",
        "    idx = text.find(label)\n",
        "    char[idx:idx+len(label)] = 1\n",
        "    if text[idx-1]==' ': char[idx-1] = 1\n",
        "    enc = tokenizer.encode(text)\n",
        "    offsets = enc.offsets\n",
        "    chard_to_token_index = []\n",
        "    start = list(char).index(1)+1\n",
        "    end_idx = start+len(label)\n",
        "    # print(start,end_idx)\n",
        "    for k in range(len(offsets)):\n",
        "        my_set = range(offsets[k][0],offsets[k][1])\n",
        "        # print(my_set)\n",
        "        if(start <= offsets[k][1] and  offsets[k][1] <= end_idx or (start in my_set and end_idx in my_set)):\n",
        "            chard_to_token_index.append(k)\n",
        "    start_tokens,end_tokens = [0]*len(input_ids[i].input_ids),[0]*len(input_ids[i].input_ids)\n",
        "    if(len(chard_to_token_index) > 1):\n",
        "        start_tokens[chard_to_token_index[0]+1] = 1\n",
        "        end_tokens[chard_to_token_index[1]+1] = 1\n",
        "    else:  \n",
        "        # print(i)\n",
        "        start_tokens[chard_to_token_index[0]+1],end_tokens[chard_to_token_index[0]+1] = 1,1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcLLXJL4qCht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08336521-b4b9-46cc-8f4e-132a1a82128a"
      },
      "source": [
        "# (input_ids,start_tokens,end_tokens)\n",
        "# Next Step is to select max_len depending on our id_size and pad accordingly\n",
        "print('Max sentence length: ', max([len(sen.input_ids) for sen in input_ids]))\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTsXsnKyUKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7760fb9e-6726-4ff0-9615-b021383ffbad"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "# Given the sequence length lets set MAX_LEN to 100\n",
        "MAX_LEN = 100\n",
        "def padding_input(ids,max_len,mask=False):\n",
        "    print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenzier.pad_token, tokenzier.pad_token_id))\n",
        "# seperate attention_masks and input_ids.\n",
        "    train_ids = [seq.input_ids for seq in ids]\n",
        "    attention_masks = [seq.attention_mask for seq in ids]\n",
        "    # input_ids = None\n",
        "    train_ids = pad_sequences(train_ids, maxlen=MAX_LEN, dtype=\"long\",value=1, truncating=\"post\", padding=\"post\")\n",
        "    if mask:\n",
        "        attention_masks = pad_sequences(attention_masks, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "        \n",
        "        return (train_ids,attention_masks)\n",
        "    return train_ids\n",
        "train_ids,masks = padding_input(input_ids,MAX_LEN,True)\n",
        "test_ids,test_masks = padding_input(test_ids,MAX_LEN,True)\n",
        "# Converting to torch tensors in order to use GPU.\n",
        "train_processed = {\n",
        "    \"ids\": torch.tensor(train_ids),\n",
        "    \"mask\": torch.tensor(masks),\n",
        "    \"start_tokens\": torch.tensor(start_tokens),\n",
        "    \"end_token\": torch.tensor(end_tokens),\n",
        "    \"token_type_ids\": torch.zeros((train.shape[0],MAX_LEN),dtype=torch.float32)\n",
        "\n",
        "}\n",
        "test_processed = {\n",
        "    \"ids\": torch.tensor(test_ids),\n",
        "    \"mask\": torch.tensor(test_masks),\n",
        "    \"token_type_ids\": torch.zeros((test.shape[0],MAX_LEN),dtype=torch.float32)\n",
        "\n",
        "}"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding token: \"<pad>\", ID: 1\n",
            "\n",
            "Padding token: \"<pad>\", ID: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_SLJbk2q8Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data pre-processing is one of the most important tasks when it comes to NLP. Next includes model \n",
        "# selection and training in order to make prediction on this new tasks\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}