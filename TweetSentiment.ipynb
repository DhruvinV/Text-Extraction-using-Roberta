{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt93MqH-lWkc",
        "colab_type": "code",
        "outputId": "781c321a-9c8d-4fa7-a29d-592b6f3703d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install regex requests"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J1qeMXUPHtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Conv2D\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOd89LBKjhvc",
        "colab_type": "code",
        "outputId": "1dd4f733-3c0a-4e8d-d01e-6285df3aad2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Data Loader\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/tweet-sentiment-extraction/'\n",
        "code = \"4/zgHVgt5EYIxQpCKdQTmi61ujDfdn_p673NBszogLYWsCZMJpSbY_WIQ\""
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL9Ing6AMSXR",
        "colab_type": "code",
        "outputId": "0ed2b5fb-3f9a-4e9f-e729-4942b699829b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def read_test():\n",
        "  path = root_path + \"test.csv\"\n",
        "  df = pd.read_csv(path)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  return shuffle(df,random_state=np.random.randint(0,10))\n",
        "def reaf_submission():\n",
        "    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n",
        "    return shuffle(test,random_state=np.random.randint(0,10))\n",
        "def read_train():\n",
        "  path = root_path + \"train.csv\"\n",
        "  df = pd.read_csv(path)\n",
        "  df['text'] = df['text'].astype(str)\n",
        "  df['selected_text']=df['selected_text'].astype(str)\n",
        "  return shuffle(df,random_state=np.random.randint(0,10))\n",
        "\n",
        "print(read_train().sample(frac=1).head(20).index.tolist)\n",
        "train_csv, test = read_train().sample(frac=1).reset_index(drop=True), read_test().sample(frac=1).reset_index(drop=True)\n",
        "train, val = train_test_split(train_csv, test_size=0.12)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method IndexOpsMixin.tolist of Int64Index([ 2346, 18279,  5558, 25447, 19862,  9863, 18583,  3120,  1004,\n",
            "            16481, 14545, 13135,  8611,  9428, 25073,  5531, 22491, 10788,\n",
            "            16982, 20782],\n",
            "           dtype='int64')>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCN6C6aYqiXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metric tested \n",
        "def jaccard(str1,str2):\n",
        "  a = set(str1.lower().split()) \n",
        "  b = set(str2.lower().split())\n",
        "  c = a.intersection(b)\n",
        "  return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPakYDyDne2U",
        "colab_type": "code",
        "outputId": "ad53e611-8cc0-4e5d-f43e-673c1e615de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "currModel = 'roberta-base'\n",
        "# using both to simplfy various methods\n",
        "tokenzier = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)\n",
        "tokenzier.save_vocabulary(root_path)\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=root_path+'vocab.json', \n",
        "    merges_file=root_path+'merges.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "# these are the ids used by our tokenizer\n",
        "sentiment = {\n",
        "    'neutral': int(tokenizer.encode('neutral').ids[0]),\n",
        "    'positive': int(tokenizer.encode('positive').ids[0]),\n",
        "    'negative':int(tokenizer.encode('negative').ids[0])\n",
        "}\n",
        "print(sentiment)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'neutral': 7974, 'positive': 1313, 'negative': 2430}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROVeSP9MBI4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The structure of our input would be of the following form [Text={Context},special_token,sentiment={Question}]\n",
        "# which is then predicted with selected_text.\n",
        "def get_input_ids(idf):\n",
        "    input_ids = []\n",
        "    for i in range(idf.shape[0]):\n",
        "        context,label = idf['text'].iloc[i].split(),idf['sentiment'].iloc[i].split()\n",
        "        encoded_input = tokenzier.encode_plus(\n",
        "                            \" \".join(context),\" \".join(label),                   # Sentence to encode.\n",
        "                            # Add '[CLS]' and '[SEP]' \n",
        "                    )\n",
        "        # encode_input returns input_ids and attention masks. \n",
        "        # Masks would be helpful latter to instruct model to avoid \n",
        "        # special tokens such as padding. \n",
        "        input_ids.append(encoded_input)\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m28T-l0Q8DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_set_ids = get_input_ids(train)\n",
        "val_set_ids = get_input_ids(val)\n",
        "test_ids = get_input_ids(test_csv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV-dG3fpIv3H",
        "colab_type": "text"
      },
      "source": [
        "The next part of data pre-processing is to come up with start and end tokens which gives start index and end index of the selected word in our encoded input. \n",
        "\n",
        "For instance, Conside the sample with\n",
        "\n",
        "text = \"Sooo SAD have to leave boston\"\n",
        "\n",
        "selected_text = \"Sooo SAD\" \n",
        "\n",
        "In this case start token is S and end token is D \n",
        "input_ids = [0, 3, 3, 11990, 560, 38457, 3, 2, 2, 2430, 2]\n",
        "\n",
        "start_token = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "end_token = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "Note: There are two ways this can be done character level and token level. I'm using character level tokenization in order to prevent loss of training samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y7Q7YEdUJY4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1_Ym_dy0eWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(input_ids[3])\n",
        "def create_labels(train2,t_ids):\n",
        "    st = []\n",
        "    et = []\n",
        "    for i in range(train2.shape[0]):\n",
        "        text,label = \" \".join(train2['text'].iloc[i].split()), \" \".join(train2['selected_text'].iloc[i].split())\n",
        "        char = np.zeros((len(text)))\n",
        "        idx = text.find(label)\n",
        "        char[idx:idx+len(label)] = 1\n",
        "        if text[idx-1]==' ': char[idx-1] = 1\n",
        "        enc = tokenizer.encode(text)\n",
        "        offsets = enc.offsets\n",
        "        chard_to_token_index = []\n",
        "        start = list(char).index(1)+1\n",
        "        end_idx = start+len(label)\n",
        "        # print(start,end_idx)\n",
        "        for k in range(len(offsets)):\n",
        "            my_set = range(offsets[k][0],offsets[k][1])\n",
        "            # print(my_set)\n",
        "            if(start <= offsets[k][1] and  offsets[k][1] <= end_idx or (start in my_set and end_idx in my_set)):\n",
        "                chard_to_token_index.append(k)\n",
        "        start_tokens,end_tokens = [0]*len(t_ids[i].input_ids),[0]*len(t_ids[i].input_ids)\n",
        "        if(len(chard_to_token_index) > 1):\n",
        "            # print(i)\n",
        "            start_tokens[chard_to_token_index[0]+1] = 1\n",
        "            end_tokens[chard_to_token_index[1]+1] = 1\n",
        "            st.append(start_tokens)\n",
        "            et.append(end_tokens)\n",
        "        else:\n",
        "            start_tokens[chard_to_token_index[0]+1],end_tokens[chard_to_token_index[0]+1] = 1,1\n",
        "            st.append(start_tokens)\n",
        "            et.append(end_tokens)\n",
        "    return st,et\n",
        "train_st,train_et = create_labels(train,train_set_ids)\n",
        "val_st,val_et = create_labels(val,val_set_ids)\n",
        "# test_st,test_et = create_labels(test_csv,train_set_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTsXsnKyUKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "7196d5ab-f808-44f3-b18b-a2d62f319da8"
      },
      "source": [
        "# Given the sequence length lets set MAX_LEN to 100\n",
        "MAX_LEN = 100\n",
        "def padding_input(ids,max_len,mask=False):\n",
        "    print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenzier.pad_token, tokenzier.pad_token_id))\n",
        "# seperate attention_masks and input_ids.\n",
        "    print(ids[0])\n",
        "    train_ids = [seq.input_ids for seq in ids]\n",
        "    attention_masks = [seq.attention_mask for seq in ids]\n",
        "    # input_ids = None\n",
        "    train_ids = pad_sequences(train_ids, maxlen=MAX_LEN, dtype=\"long\",value=1, truncating=\"post\", padding=\"post\")\n",
        "    if mask:\n",
        "        attention_masks = pad_sequences(attention_masks, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\")\n",
        "        \n",
        "        return (train_ids,attention_masks)\n",
        "    return train_ids\n",
        "train_ids, train_masks = padding_input(train_set_ids,MAX_LEN,True)\n",
        "val_ids,val_masks = padding_input(val_set_ids,MAX_LEN,True)\n",
        "test_ids,test_masks = padding_input(test_ids,MAX_LEN,True)\n",
        "# Converting to torch tensors in order to use GPU."
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding token: \"<pad>\", ID: 1\n",
            "{'input_ids': [0, 939, 3996, 169, 33175, 219, 350, 203, 28, 2802, 352, 14798, 361, 4197, 698, 452, 8, 939, 524, 45, 2602, 9, 14, 754, 4, 30016, 5998, 2, 2, 2430, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "\n",
            "Padding token: \"<pad>\", ID: 1\n",
            "{'input_ids': [0, 18134, 29411, 939, 11269, 139, 939, 376, 11, 1029, 329, 5, 3778, 1682, 164, 639, 5, 10722, 53, 122, 24, 34, 283, 124, 4005, 11, 127, 2931, 122, 15, 5, 1929, 29784, 2, 2, 7974, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "\n",
            "Padding token: \"<pad>\", ID: 1\n",
            "{'input_ids': [0, 14223, 2841, 119, 821, 1942, 12846, 2677, 3209, 3204, 11, 545, 360, 4, 939, 524, 98, 2283, 4, 2, 2, 1313, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3vLf_T4o-5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = {\n",
        "    \"ids\": torch.tensor(train_ids,dtype=torch.float),\n",
        "    \"att_mask\": torch.tensor(train_masks,dtype=torch.float),\n",
        "    \"token_type_ids\": torch.zeros((train.shape[0],MAX_LEN),dtype=torch.float)\n",
        "}\n",
        "val_data = {\n",
        "    \"ids\": torch.tensor(val_ids,dtype=torch.float),\n",
        "    \"att_mask\": torch.tensor(val_masks,dtype=torch.float),\n",
        "    \"token_type_ids\": torch.zeros((val.shape[0],MAX_LEN),dtype=torch.float)\n",
        "\n",
        "}\n",
        "test_data = {\n",
        "    \"ids\": torch.tensor(test_ids,dtype=torch.float),\n",
        "    \"att_mask\": torch.tensor(test_masks,dtype=torch.float),\n",
        "    \"token_type_ids\": torch.zeros((test.shape[0],MAX_LEN),dtype=torch.float)\n",
        "}\n",
        "train_label = {\n",
        "    \"start_tokens\":torch.tensor(pad_sequences(train_st,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.float),\n",
        "    \"end_tokens\": torch.tensor(pad_sequences(train_et,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.float),\n",
        "}\n",
        "val_label = {\n",
        "    \"start_tokens\":torch.tensor(pad_sequences(val_st,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.float),\n",
        "    \"end_tokens\": torch.tensor(pad_sequences(val_et,maxlen=MAX_LEN, dtype=\"long\", \n",
        "                            value=0, truncating=\"post\", padding=\"post\"),dtype=torch.float),\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcIrDf4GVeXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d8168008-5e98-4be5-90a4-4b0dec9961e2"
      },
      "source": [
        "# Create the DataLoader for our training set.\n",
        "print(val_data[\"ids\"].shape,val_data[\"att_mask\"].shape,val_data[\"token_type_ids\"].shape)\n",
        "print(val_label[\"start_tokens\"].shape,val_label[\"end_tokens\"].shape)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3298, 100]) torch.Size([3298, 100]) torch.Size([3298, 100])\n",
            "torch.Size([3298, 100]) torch.Size([3298, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f9zqCArzQ6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "batch_size = 64\n",
        "\n",
        "train_data = TensorDataset(train_data[\"ids\"], train_data[\"att_mask\"], train_data[\"token_type_ids\"],train_label[\"start_tokens\"],train_label[\"end_tokens\"])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(val_data[\"ids\"], val_data[\"att_mask\"], val_data[\"token_type_ids\"],val_label[\"start_tokens\"],val_label[\"end_tokens\"])\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSIENFdN4Dot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data pre-processing is one of the most important tasks when it comes to NLP. Next includes model \n",
        "# selection and training in order to make prediction on this new task.\n",
        "# Model\n",
        "from transformers import RobertaModel,RobertaConfig\n",
        "class SentimentAnalysis(nn.Module):\n",
        "    def __init__(self,configs):\n",
        "        super(SentimentAnalysis,self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(root_path+'roberta-base-weights.bin',config=configs)\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Conv1d(768,768,kernel_size=1)  \n",
        "        )\n",
        "        self.avgpool  =  nn.AvgPool1d(kernel_size=5)\n",
        "        self.linear = nn.Linear(153,2)\n",
        "        self.linear2 = nn.Linear(2,1)\n",
        "    def forward(self,token_ids,att_masks,type_ids):\n",
        "        outputs[0] = self.roberta(\n",
        "            input_ids=token_ids,\n",
        "            attention_mask=att_masks,\n",
        "            token_type_ids=type_ids,\n",
        "                        )\n",
        "        bs = roberta.shape[0]   \n",
        "        sl = self.container(torch.transpose(bs,dim0=1,dim1=2))\n",
        "        avgpool = self.avgpool(torch.transpose(sq,dim0=1,dim1=2))\n",
        "        start_logits = self.linear2(nn.functional.softmax(self.linear(avgpool),dim=2))\n",
        "        el = self.container(torch.transpose(bs,dim0=1,dim1=2))\n",
        "        avgpool = self.avgpool(self.avgpool(torch.transpose(el,dim0=1,dim1=2)))\n",
        "        end_logits = self.linear2(nn.function.softmax(self.linear(avgpool),dim=2))\n",
        "        start_logits = nn.functional.softmax(torch.flatten(start_logits,start_dim=1,end_dim=2),dim=1)\n",
        "        end_logits = nn.functional.softmax(torch.flatten(start_logits,start_dim=1,end_dim=2),dim=1)\n",
        "        return start_logits,end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl3raGFWJbdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SentimentAnalysis(configs=root_path+'config.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgxvLxxciIzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b528148e-a14e-4ef7-cfc0-e9ecf9ab3765"
      },
      "source": [
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "# print(model)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentAnalysis(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (container): Sequential(\n",
              "    (0): Dropout(p=0.1, inplace=False)\n",
              "    (1): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
              "  )\n",
              "  (avgpool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
              "  (linear): Linear(in_features=153, out_features=2, bias=True)\n",
              "  (linear2): Linear(in_features=2, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zi9T_IFFZ0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from transformers import get_linear_schedule_with_warmup\n",
        "# from transformers import AdamW\n",
        "# import random\n",
        "# seed_val = 42\n",
        "# random.seed(seed_val)\n",
        "# np.random.seed(seed_val)\n",
        "# torch.manual_seed(seed_val)\n",
        "# torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# def train(args):\n",
        "#     optimizer = AdamW(model.parameters(),\n",
        "#                   lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "#                   eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
        "#                   weight_decay = 0.1\n",
        "#                 )\n",
        "#     epochs = 10\n",
        "#     scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "#                                         num_warmup_steps = 0, \n",
        "#                                         num_training_steps = total_steps)\n",
        "#     loss_values\n",
        "#     for epoch_i in range(0, epochs):\n",
        "#         t0 = time.time()\n",
        "#         total_loss = 0\n",
        "#         model.train()\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY4HJvFaHFkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# args = AttrDict()\n",
        "# args_dict = {\n",
        "#               'gpu':True, \n",
        "#               'valid':False, \n",
        "#               'checkpoint':\"\", \n",
        "#             #   'batch_size':32, \n",
        "#               'epochs':20, \n",
        "#               'seed':8,\n",
        "# }"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}